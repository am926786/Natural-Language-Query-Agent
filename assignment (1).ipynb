{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7a8675f9-dfef-4f0e-b3bd-06bf89137a1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in c:\\users\\lenovo\\.anaconda\\lib\\site-packages (4.41.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\lenovo\\.anaconda\\lib\\site-packages (from transformers) (3.13.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.0 in c:\\users\\lenovo\\.anaconda\\lib\\site-packages (from transformers) (0.23.1)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\lenovo\\.anaconda\\lib\\site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\lenovo\\.anaconda\\lib\\site-packages (from transformers) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\lenovo\\.anaconda\\lib\\site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\lenovo\\.anaconda\\lib\\site-packages (from transformers) (2023.10.3)\n",
      "Requirement already satisfied: requests in c:\\users\\lenovo\\.anaconda\\lib\\site-packages (from transformers) (2.31.0)\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in c:\\users\\lenovo\\.anaconda\\lib\\site-packages (from transformers) (0.19.1)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in c:\\users\\lenovo\\.anaconda\\lib\\site-packages (from transformers) (0.4.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\lenovo\\.anaconda\\lib\\site-packages (from transformers) (4.66.4)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\lenovo\\.anaconda\\lib\\site-packages (from huggingface-hub<1.0,>=0.23.0->transformers) (2023.10.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\lenovo\\.anaconda\\lib\\site-packages (from huggingface-hub<1.0,>=0.23.0->transformers) (4.9.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\lenovo\\.anaconda\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\lenovo\\.anaconda\\lib\\site-packages (from requests->transformers) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\lenovo\\.anaconda\\lib\\site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\lenovo\\.anaconda\\lib\\site-packages (from requests->transformers) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\lenovo\\.anaconda\\lib\\site-packages (from requests->transformers) (2024.2.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers\n",
    "# Additional libraries as needed (e.g., requests, pandas)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1d1b424d-dcb7-4cba-8a6d-2ec104130e7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer 1: BERT and GPT-3\n",
      "Answer 2: feed-forward layers\n",
      "Answer 3: I'm sorry, I don't have enough information to answer that question.\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Load the pre-trained model\n",
    "qa_pipeline = pipeline(\"question-answering\", model=\"distilbert-base-uncased-distilled-squad\")\n",
    "\n",
    "# Example data (replace with actual data loading logic)\n",
    "lecture_notes = {\n",
    "    \"introductory\": \"\"\"\n",
    "        This section covers the basics of language models and their applications.\n",
    "        Language models like BERT and GPT-3 have revolutionized natural language processing.\n",
    "        \"\"\",\n",
    "    \"advanced\": \"\"\"\n",
    "        Advanced topics include transfer learning, fine-tuning, and multi-task learning in LLMs.\n",
    "        Transformers have shown state-of-the-art performance across various NLP tasks.\n",
    "        \"\"\"\n",
    "}\n",
    "\n",
    "architecture_table = {\n",
    "    \"transformer\": \"\"\"\n",
    "        The transformer architecture consists of self-attention mechanisms and feed-forward layers.\n",
    "        It has been pivotal in achieving impressive results in NLP tasks such as translation and summarization.\n",
    "        \"\"\",\n",
    "    \"bert\": \"\"\"\n",
    "        BERT (Bidirectional Encoder Representations from Transformers) is designed to pre-train deep bidirectional representations by jointly conditioning on both left and right context.\n",
    "        \"\"\"\n",
    "}\n",
    "\n",
    "def answer_question(question):\n",
    "    # Determine which data source to query based on question type\n",
    "    if \"milestone model architectures\" in question:\n",
    "        relevant_text = lecture_notes[\"introductory\"] + \"\\n\" + architecture_table[\"transformer\"]\n",
    "    elif \"transformer block\" in question:\n",
    "        relevant_text = architecture_table[\"transformer\"]\n",
    "    else:\n",
    "        # Handle other types of questions appropriately\n",
    "        relevant_text = \"\"  # Placeholder for unknown questions\n",
    "\n",
    "    # Ensure relevant_text is not empty before calling qa_pipeline\n",
    "    if relevant_text.strip() == \"\":\n",
    "        return \"I'm sorry, I don't have enough information to answer that question.\"\n",
    "\n",
    "    # Use the model to generate an answer\n",
    "    answer = qa_pipeline({\n",
    "        \"question\": question,\n",
    "        \"context\": relevant_text\n",
    "    })\n",
    "    \n",
    "    return answer[\"answer\"]\n",
    "\n",
    "# Example queries\n",
    "query1 = \"What are some milestone model architectures and papers in the last few years?\"\n",
    "query2 = \"What are the layers in a transformer block?\"\n",
    "query3 = \"Tell me about datasets used to train LLMs and how theyâ€™re cleaned.\"\n",
    "\n",
    "# Get answers\n",
    "answer1 = answer_question(query1)\n",
    "answer2 = answer_question(query2)\n",
    "answer3 = answer_question(query3)\n",
    "\n",
    "# Print answers\n",
    "print(\"Answer 1:\", answer1)\n",
    "print(\"Answer 2:\", answer2)\n",
    "print(\"Answer 3:\", answer3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c4ca27e-cc60-4dd5-beb6-bfec435f2f5f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
